{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa991e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PS : Text Analytics\n",
    "    # 1. Extract Sample document and apply following document preprocessing\n",
    "    # methods:Tokenization, POS Tagging, stop words removal, Stemming and\n",
    "    # Lemmatization.\n",
    "    # 2. Create representation of document by calculating Term Frequency and Inverse\n",
    "    # Document Frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8f40d854",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'spacy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 10\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstem\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PorterStemmer, WordNetLemmatizer\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtokenize\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m word_tokenize\n\u001b[1;32m---> 10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mspacy\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeature_extraction\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtext\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TfidfVectorizer\n\u001b[0;32m     13\u001b[0m nltk\u001b[38;5;241m.\u001b[39mdownload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpunkt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'spacy'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "import spacy\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5f29bac9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenization is the first step in text analytics. The process of breaking down a text paragraph into smaller chunks such as words or sentences is called Tokenization.\n"
     ]
    }
   ],
   "source": [
    "text=\"Tokenization is the first step in text analytics. The process of breaking down a text paragraph into smaller chunks such as words or sentences is called Tokenization.\"\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33bffd8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Tokenization\n",
    "tokens = word_tokenize(text)\n",
    "print(\"\\nTokenization:\\n\", tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e13f6ffe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "POS Tagging:\n",
      " [('Tokenization', 'NN'), ('is', 'VBZ'), ('the', 'DT'), ('first', 'JJ'), ('step', 'NN'), ('in', 'IN'), ('text', 'JJ'), ('analytics', 'NNS'), ('.', '.'), ('The', 'DT'), ('process', 'NN'), ('of', 'IN'), ('breaking', 'VBG'), ('down', 'RP'), ('a', 'DT'), ('text', 'NN'), ('paragraph', 'NN'), ('into', 'IN'), ('smaller', 'JJR'), ('chunks', 'NNS'), ('such', 'JJ'), ('as', 'IN'), ('words', 'NNS'), ('or', 'CC'), ('sentences', 'NNS'), ('is', 'VBZ'), ('called', 'VBN'), ('Tokenization', 'NN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "# 2. POS Tagging\n",
    "pos_tags = nltk.pos_tag(tokens)\n",
    "print(\"\\nPOS Tagging:\\n\", pos_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f707a4be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tokens after Stop Words Removal:\n",
      " ['Tokenization', 'first', 'step', 'text', 'analytics', '.', 'process', 'breaking', 'text', 'paragraph', 'smaller', 'chunks', 'words', 'sentences', 'called', 'Tokenization', '.']\n"
     ]
    }
   ],
   "source": [
    "# 3. Stop Words Removal\n",
    "stop_words = set(stopwords.words('english'))\n",
    "tokens_without_sw = [word for word in tokens if word.lower() not in stop_words]\n",
    "print(\"\\nTokens after Stop Words Removal:\\n\", tokens_without_sw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4b070ea4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stemming:\n",
      " ['token', 'first', 'step', 'text', 'analyt', '.', 'process', 'break', 'text', 'paragraph', 'smaller', 'chunk', 'word', 'sentenc', 'call', 'token', '.']\n"
     ]
    }
   ],
   "source": [
    "# 4. Stemming\n",
    "stemmer = PorterStemmer()\n",
    "stemmed_tokens = [stemmer.stem(word) for word in tokens_without_sw]\n",
    "print(\"\\nStemming:\\n\", stemmed_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3038a195",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Lemmatization:\n",
      " ['Tokenization', 'first', 'step', 'text', 'analytics', '.', 'process', 'break', 'text', 'paragraph', 'smaller', 'chunk', 'word', 'sentence', 'call', 'Tokenization', '.']\n"
     ]
    }
   ],
   "source": [
    "# 5. Lemmatization\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized_tokens = [lemmatizer.lemmatize(word, pos='v') for word in tokens_without_sw]\n",
    "print(\"\\nLemmatization:\\n\", lemmatized_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02311330",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TF-IDF Representation:\n",
      "\n",
      "Term: analytics TF-IDF: 0.1715\n",
      "Term: as TF-IDF: 0.1715\n",
      "Term: breaking TF-IDF: 0.1715\n",
      "Term: called TF-IDF: 0.1715\n",
      "Term: chunks TF-IDF: 0.1715\n",
      "Term: down TF-IDF: 0.1715\n",
      "Term: first TF-IDF: 0.1715\n",
      "Term: in TF-IDF: 0.1715\n",
      "Term: into TF-IDF: 0.1715\n",
      "Term: is TF-IDF: 0.343\n",
      "Term: of TF-IDF: 0.1715\n",
      "Term: or TF-IDF: 0.1715\n",
      "Term: paragraph TF-IDF: 0.1715\n",
      "Term: process TF-IDF: 0.1715\n",
      "Term: sentences TF-IDF: 0.1715\n",
      "Term: smaller TF-IDF: 0.1715\n",
      "Term: step TF-IDF: 0.1715\n",
      "Term: such TF-IDF: 0.1715\n",
      "Term: text TF-IDF: 0.343\n",
      "Term: the TF-IDF: 0.343\n",
      "Term: tokenization TF-IDF: 0.343\n",
      "Term: words TF-IDF: 0.1715\n"
     ]
    }
   ],
   "source": [
    "# 6. TF-IDF Representation : Appropriate\n",
    "documents = [text]  # List of one document\n",
    "\n",
    "# Initialize TF-IDF Vectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit and transform\n",
    "tfidf_matrix = vectorizer.fit_transform(documents)\n",
    "\n",
    "# Get feature names\n",
    "terms = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Convert to array\n",
    "tfidf_array = tfidf_matrix.toarray()\n",
    "\n",
    "# Print TF-IDF scores\n",
    "print(\"\\nTF-IDF Representation:\\n\")\n",
    "for i in range(len(terms)):\n",
    "    print(\"Term:\", terms[i], \"TF-IDF:\", round(tfidf_array[0][i], 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa53c3f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Custom TF-IDF Representation:\n",
      "\n",
      "Term: Tokenization, TF-IDF: 2.0000\n",
      "Term: is, TF-IDF: 2.0000\n",
      "Term: the, TF-IDF: 1.0000\n",
      "Term: first, TF-IDF: 1.0000\n",
      "Term: step, TF-IDF: 1.0000\n",
      "Term: in, TF-IDF: 1.0000\n",
      "Term: text, TF-IDF: 2.0000\n",
      "Term: analytics, TF-IDF: 1.0000\n",
      "Term: ., TF-IDF: 2.0000\n",
      "Term: The, TF-IDF: 1.0000\n",
      "Term: process, TF-IDF: 1.0000\n",
      "Term: of, TF-IDF: 1.0000\n",
      "Term: breaking, TF-IDF: 1.0000\n",
      "Term: down, TF-IDF: 1.0000\n",
      "Term: a, TF-IDF: 1.0000\n",
      "Term: paragraph, TF-IDF: 1.0000\n",
      "Term: into, TF-IDF: 1.0000\n",
      "Term: smaller, TF-IDF: 1.0000\n",
      "Term: chunks, TF-IDF: 1.0000\n",
      "Term: such, TF-IDF: 1.0000\n",
      "Term: as, TF-IDF: 1.0000\n",
      "Term: words, TF-IDF: 1.0000\n",
      "Term: or, TF-IDF: 1.0000\n",
      "Term: sentences, TF-IDF: 1.0000\n",
      "Term: called, TF-IDF: 1.0000\n"
     ]
    }
   ],
   "source": [
    "# 6. TF-IDF Representation : Approx\n",
    "def calculate_tf_idf(documents):\n",
    "    # Tokenize the document\n",
    "    tokens = word_tokenize(documents[0])\n",
    "    \n",
    "    # Calculate Term Frequency (TF)\n",
    "    tf = {}\n",
    "    for word in tokens:\n",
    "        if word in tf:\n",
    "            tf[word] += 1\n",
    "        else:\n",
    "            tf[word] = 1\n",
    "    \n",
    "    # Calculate Inverse Document Frequency (IDF)\n",
    "    n_documents = len(documents)\n",
    "    idf = {}\n",
    "    for word in tokens:\n",
    "        count = sum(1 for doc in documents if word in doc)\n",
    "        idf[word] = n_documents / count\n",
    "    \n",
    "    # Calculate TF-IDF\n",
    "    tf_idf = {word: tf[word] * idf[word] for word in tf}\n",
    "    \n",
    "    return tf_idf\n",
    "\n",
    "# Example usage\n",
    "documents = [text]\n",
    "tf_idf = calculate_tf_idf(documents)\n",
    "print(\"\\nCustom TF-IDF Representation:\\n\")\n",
    "for word, score in tf_idf.items():\n",
    "    print(f\"Term: {word}, TF-IDF: {score:.4f}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
